{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "import numpy as np\n",
    "from scipy.signal import periodogram\n",
    "import statsmodels.api as sm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretic part (20 pts, 5 pts each)\n",
    "\n",
    "Multiple choice questions: please select all that applies and explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (Autocorrelation). \n",
    "The autocorrelation plot of the daily time-series has local peaks at t=7,14,21,28 etc.. How would you interpret that?\n",
    "\n",
    "A. The time-series reaches its maximum on the days 7,14,21,28...\n",
    "\n",
    "B. The time-series reaches its minimum on the days 7,14,21,28...\n",
    "\n",
    "C. The time-series is likely to have a periodic pattern with a period of 7 days\n",
    "\n",
    "D. The time-series is likely to have 7 periods per day\n",
    "\n",
    "E. The appropriate AR model for the time-series should have at least 7 terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - C \n",
    "The autocorrelation plot of the daily time-series having local peaks at t=7, 14, 21, 28, etc. suggests that the time-series is likely to have a periodic pattern with a period of 7 days. This means that the values of the time-series are repeating themselves every 7 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (Stationarity).\n",
    "\n",
    "Which of the following time-series models are stationary:\n",
    "\n",
    "A. Linear trend\n",
    "\n",
    "B. AR(1) model\n",
    "\n",
    "C. White noise\n",
    "\n",
    "D. Random walk\n",
    "\n",
    "E. ARMA(1,2) model\n",
    "\n",
    "F. ARIMA(1,1,1) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - C and E\n",
    "White noise is stationary because it has constant mean and variance and the autocorrelation between any two observations in the series is zero. ARMA(1,2) model is also stationary under certain conditions. For example, if the roots of the characteristic equation lie outside the unit circle, then the ARMA(1,2) model is stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (PCA). \n",
    "Which of the following statements regarding the model dimensionality reduction through Principal Component Analysis (PCA) are true:\n",
    "\n",
    "A. Leading principal components of the features are the most efficient for modeling the output variable.\n",
    "\n",
    "B. Principal components of the standardized features are uncorrelated and this way less exposed to multicollinearity.\n",
    "\n",
    "C. The model using principal components of the features can't overfit.\n",
    "\n",
    "D. Feature selection based on the principal components of the features is often more efficient in preventing overfitting comparered the feature selection over the original features.\n",
    "\n",
    "E. Principal components are harder to interpret compared to the original features making the PCA regresssion model less interpretable compared to the regression model using original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - B, D, and E\n",
    "B. Standardizing the features before applying PCA ensures that the resulting principal components are not influenced by differences in the scale or units of the original features. This can improve the stability of the PCA results and make them less sensitive to multicollinearity.\n",
    "\n",
    "D. This is because PCA reduces the dimensionality of the feature space while retaining most of the relevant information, which can help to prevent overfitting and improve the generalization of the model.\n",
    "\n",
    "E. This is because the principal components are linear combinations of the original features, and the coefficients in the regression equation represent the contributions of each principal component rather than the original features themselves. However, this interpretability tradeoff may be acceptable if the reduction in dimensionality and the resulting increase in model efficiency and generalization are deemed more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (MapReduce). \n",
    "\n",
    "What is true about MapReduce:\n",
    "\n",
    "A. MapReduce is a Python module enabling parallel computing\n",
    "\n",
    "B. Using MapReduce approach makes the code more suitable for parallel computing.\n",
    "\n",
    "C. MapReduce code always runs faster compared to the code using more traditional approaches, like loops or list comprehensions.\n",
    "\n",
    "D. MapReduce code will always efficiently run on multiple cores of you CPU or multiple machines within your cluster if available.\n",
    "\n",
    "E. Multiprocessing and PySpark efficient alternatives to MapReduce.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer - BCDE\n",
    "MapReduce is a programming paradigm for distributed computing on large datasets. It allows for parallel processing by dividing the input data into smaller chunks that can be processed independently by multiple nodes in a cluster, and then aggregating the results. Using the MapReduce approach can make the code more suitable for parallel computing because it provides a framework for handling the parallelization of the code and the distribution of data across the nodes in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice part: Taxi ridership from JFK to other taxi zones prediction.\n",
    "This project is an example of applying PCA to predict hourly yellow taxi ridership at the taxi zone level. Modeling taxi ridership at a fine spatial and temporal granularity is challenging due to the low signal-to-noise ratio and high dimensionality. In this case, dimension reduction essential in feature engineering. This project has five steps: data downloading, data preprocessing, baseline modeling, feature engineering, and RandomForest modeling.\n",
    "\n",
    "Let's start with data downloading. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data downloading (5pts)\n",
    "Design a function to download yellow taxi data from 2017-01-01 to 2018-12-31 at https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = 'taxidata'\n",
    "if os.path.exists(dataDir):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(dataDir)\n",
    "Years = [2017,2018]\n",
    "Months = range(1,13)\n",
    "VehicleTypes = ['yellow']\n",
    "\n",
    "def getUrl(cabtype,year,month):\n",
    "    baseUrl = 'https://d37ci6vzurychx.cloudfront.net/trip-data/'\n",
    "    \n",
    "    month = str(month).zfill(2)\n",
    "    fileName = '%s_tripdata_%s-%s.parquet'%(cabtype,year,month)\n",
    "        \n",
    "    return baseUrl + fileName, fileName        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: yellow_tripdata_2017-01.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-02.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-03.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-04.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-05.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-06.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-07.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-08.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-09.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-10.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-11.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2017-12.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-01.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-02.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-03.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-04.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-05.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-06.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-07.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-08.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-09.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-10.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-11.parquet\n",
      "file exists\n",
      "Downloading: yellow_tripdata_2018-12.parquet\n",
      "file exists\n"
     ]
    }
   ],
   "source": [
    "for year in Years:\n",
    "    for month in Months:\n",
    "        for cabtype in VehicleTypes:\n",
    "            url, fileName = getUrl(cabtype,year,month)\n",
    "            \n",
    "            print(\"Downloading: \"+str(fileName))\n",
    "            \n",
    "            if fileName in os.listdir(dataDir):\n",
    "                print(\"file exists\")\n",
    "                continue\n",
    "            \n",
    "            filePath = os.path.join(dataDir, fileName)\n",
    "            try:\n",
    "                urllib.request.urlretrieve(url, filePath)\n",
    "            except:\n",
    "                # if fails remove the incomplete file\n",
    "                os.remove(filePath)\n",
    "                try:\n",
    "                    # start again after a delay of 2 min\n",
    "                    time.sleep(60*2)\n",
    "                    urllib.request.urlretrieve(url, filePath)\n",
    "                except:\n",
    "                    print(\"Download this file later!\")\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing (10 pts, 7 for dask, 3 for sanity check)\n",
    "Use dask to aggregate all months' records into one dataframe, and aggregate dataset by date and hour to get the ridership from JFK to each taxi zone each hour. The expected output has columns: date, hour, drop-off location 1, drop-off location 2, etc. \n",
    "\n",
    "Hint: \n",
    "1. JFK taxi zone id is 132.\n",
    "2. time column should be the pickup time, and ridership is passenger count.\n",
    "3. Try read_csv(\"*.csv\") to read all csv file in a folder \n",
    "4. files in 2017 and 2018 have different columns; apply argument usecols to select desired columns.\n",
    "5. using .compute() function to convert processed dask dataframe to pandas dataframe for further modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "# read file: 'read_csv()' works just like pandas\n",
    "%time df = dd.read_parquet('yellow_tripdata_2017-01.parquet')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.92 s\n",
      "Wall time: 2.02 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:32:05</td>\n",
       "      <td>2017-01-01 00:37:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:43:25</td>\n",
       "      <td>2017-01-01 00:47:42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:49:10</td>\n",
       "      <td>2017-01-01 00:53:53</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>237</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:36:42</td>\n",
       "      <td>2017-01-01 00:41:09</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:07:41</td>\n",
       "      <td>2017-01-01 00:18:16</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2017-01-01 00:32:05   2017-01-01 00:37:48                1   \n",
       "1         1  2017-01-01 00:43:25   2017-01-01 00:47:42                2   \n",
       "2         1  2017-01-01 00:49:10   2017-01-01 00:53:53                2   \n",
       "3         1  2017-01-01 00:36:42   2017-01-01 00:41:09                1   \n",
       "4         1  2017-01-01 00:07:41   2017-01-01 00:18:16                1   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.2           1                  N           140           236   \n",
       "1            0.7           1                  N           237           140   \n",
       "2            0.8           1                  N           140           237   \n",
       "3            1.1           1                  N            41            42   \n",
       "4            3.0           1                  N            48           263   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          6.5    0.5      0.5         0.0           0.0   \n",
       "1             2          5.0    0.5      0.5         0.0           0.0   \n",
       "2             2          5.5    0.5      0.5         0.0           0.0   \n",
       "3             2          6.0    0.5      0.5         0.0           0.0   \n",
       "4             2         11.0    0.5      0.5         0.0           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount congestion_surcharge airport_fee  \n",
       "0                    0.3           7.8                 None        None  \n",
       "1                    0.3           6.3                 None        None  \n",
       "2                    0.3           6.8                 None        None  \n",
       "3                    0.3           7.3                 None        None  \n",
       "4                    0.3          12.3                 None        None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check time with pandas \n",
    "%time df_demo= pd.read_parquet('yellow_tripdata_2017-01.parquet')\n",
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:32:05</td>\n",
       "      <td>2017-01-01 00:37:48</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:43:25</td>\n",
       "      <td>2017-01-01 00:47:42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:49:10</td>\n",
       "      <td>2017-01-01 00:53:53</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>237</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:36:42</td>\n",
       "      <td>2017-01-01 00:41:09</td>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-01-01 00:07:41</td>\n",
       "      <td>2017-01-01 00:18:16</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2017-01-01 00:32:05   2017-01-01 00:37:48                1   \n",
       "1         1  2017-01-01 00:43:25   2017-01-01 00:47:42                2   \n",
       "2         1  2017-01-01 00:49:10   2017-01-01 00:53:53                2   \n",
       "3         1  2017-01-01 00:36:42   2017-01-01 00:41:09                1   \n",
       "4         1  2017-01-01 00:07:41   2017-01-01 00:18:16                1   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            1.2           1                  N           140           236   \n",
       "1            0.7           1                  N           237           140   \n",
       "2            0.8           1                  N           140           237   \n",
       "3            1.1           1                  N            41            42   \n",
       "4            3.0           1                  N            48           263   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          6.5    0.5      0.5         0.0           0.0   \n",
       "1             2          5.0    0.5      0.5         0.0           0.0   \n",
       "2             2          5.5    0.5      0.5         0.0           0.0   \n",
       "3             2          6.0    0.5      0.5         0.0           0.0   \n",
       "4             2         11.0    0.5      0.5         0.0           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    0.3           7.8                  <NA>         <NA>  \n",
       "1                    0.3           6.3                  <NA>         <NA>  \n",
       "2                    0.3           6.8                  <NA>         <NA>  \n",
       "3                    0.3           7.3                  <NA>         <NA>  \n",
       "4                    0.3          12.3                  <NA>         <NA>  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_parquet('yellow_tripdata_2017-*.parquet', dtype={'trip_distance': float,\n",
    "                        'total_amount': float, 'tolls_amount':float, 'RatecodeID': float, 'VendorID': float, \n",
    "                                                     'passenger_count': float, 'payment_type':float, \n",
    "                                                     'PULocationID':int, 'DOLocationID':int})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:21:05</td>\n",
       "      <td>2018-01-01 00:24:23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:44:55</td>\n",
       "      <td>2018-01-01 01:03:05</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:08:26</td>\n",
       "      <td>2018-01-01 00:14:21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>262</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:20:22</td>\n",
       "      <td>2018-01-01 00:52:51</td>\n",
       "      <td>1</td>\n",
       "      <td>10.2</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>257</td>\n",
       "      <td>2</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>34.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:09:18</td>\n",
       "      <td>2018-01-01 00:27:06</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         1  2018-01-01 00:21:05   2018-01-01 00:24:23                1   \n",
       "1         1  2018-01-01 00:44:55   2018-01-01 01:03:05                1   \n",
       "2         1  2018-01-01 00:08:26   2018-01-01 00:14:21                2   \n",
       "3         1  2018-01-01 00:20:22   2018-01-01 00:52:51                1   \n",
       "4         1  2018-01-01 00:09:18   2018-01-01 00:27:06                2   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0            0.5           1                  N            41            24   \n",
       "1            2.7           1                  N           239           140   \n",
       "2            0.8           1                  N           262           141   \n",
       "3           10.2           1                  N           140           257   \n",
       "4            2.5           1                  N           246           239   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2          4.5    0.5      0.5        0.00           0.0   \n",
       "1             2         14.0    0.5      0.5        0.00           0.0   \n",
       "2             1          6.0    0.5      0.5        1.00           0.0   \n",
       "3             2         33.5    0.5      0.5        0.00           0.0   \n",
       "4             1         12.5    0.5      0.5        2.75           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
       "0                    0.3          5.80                   NaN          NaN  \n",
       "1                    0.3         15.30                   NaN          NaN  \n",
       "2                    0.3          8.30                   NaN          NaN  \n",
       "3                    0.3         34.80                   NaN          NaN  \n",
       "4                    0.3         16.55                   NaN          NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = dd.read_parquet('yellow_tripdata_2018-*.parquet', dtype={'trip_distance': float,\n",
    "                        'total_amount': float, 'tolls_amount':float, 'RatecodeID': float, 'VendorID': float, \n",
    "                                                     'passenger_count': float, 'payment_type':float, \n",
    "                                                     'PULocationID':int, 'DOLocationID':int})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "# Read all parquet files for 2017 dataset\n",
    "df_17 = dd.read_parquet('yellow_tripdata_2017-*.parquet', dtype={'trip_distance': float,\n",
    "                        'total_amount': float, 'tolls_amount':float, 'RatecodeID': float, 'VendorID': float, \n",
    "                                                     'passenger_count': float, 'payment_type':float, \n",
    "                                                     'PULocationID':int, 'DOLocationID':int},\n",
    "                     usecols=['tpep_pickup_datetime', 'passenger_count', 'PULocationID', 'DOLocationID'])\n",
    "\n",
    "# Filter the records with pickup from JFK (zone 132)\n",
    "df_jfk_17 = df_17[df_17['PULocationID'] == 132]\n",
    "\n",
    "# Extract date and hour from pickup time\n",
    "df_jfk_17['date'] = df_jfk_17['tpep_pickup_datetime'].dt.date\n",
    "df_jfk_17['hour'] = df_jfk_17['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "df_jfk_17 = df_jfk_17.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  hour  DOLocationID  passenger_count\n",
       "0  2017-01-01     0             4                1\n",
       "1  2017-01-01     0             7                2\n",
       "2  2017-01-01     0            10                7\n",
       "3  2017-01-01     0            12                1\n",
       "4  2017-01-01     0            13               13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the desired columns\n",
    "df_jfk_17 = df_jfk_17[['date', 'hour', 'DOLocationID', 'passenger_count']]\n",
    "\n",
    "# Convert the dask dataframe to pandas dataframe\n",
    "#df_jfk_17_pandas = df_jfk_17.compute()\n",
    "\n",
    "df_agg_17 = df_jfk_17.groupby(['date', 'hour', 'DOLocationID']).sum()[['passenger_count']].reset_index()\n",
    "\n",
    "df_agg_17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metadata inference failed in `ge`.\n\nOriginal error is below:\n------------------------\nTypeError(\"'>=' not supported between instances of 'str' and 'Timestamp'\")\n\nTraceback:\n---------\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py\", line 177, in raise_on_meta_error\n    yield\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py\", line 5756, in elemwise\n    meta = partial_by_order(*parts, function=op, other=other)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\dask\\utils.py\", line 1225, in partial_by_order\n    return function(*args2, **kwargs)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\", line 72, in new_method\n    return method(self, other)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\", line 62, in __ge__\n    return self._cmp_method(other, operator.ge)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 6243, in _cmp_method\n    res_values = ops.comparison_op(lvalues, rvalues, op)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 287, in comparison_op\n    res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 75, in comp_method_OBJECT_ARRAY\n    result = libops.scalar_compare(x.ravel(), y, op)\n  File \"pandas\\_libs\\ops.pyx\", line 107, in pandas._libs.ops.scalar_compare\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:177\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[1;34m(funcname, udf)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:5756\u001b[0m, in \u001b[0;36melemwise\u001b[1;34m(op, meta, out, transform_divisions, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(op)):\n\u001b[1;32m-> 5756\u001b[0m         meta \u001b[38;5;241m=\u001b[39m \u001b[43mpartial_by_order\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5758\u001b[0m result \u001b[38;5;241m=\u001b[39m new_dd_object(graph, _name, meta, divisions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\utils.py:1225\u001b[0m, in \u001b[0;36mpartial_by_order\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1224\u001b[0m     args2\u001b[38;5;241m.\u001b[39minsert(i, arg)\n\u001b[1;32m-> 1225\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py:62\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 6243\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(lvalues\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 287\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mcomp_method_OBJECT_ARRAY\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[1;34m(op, x, y)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mlibops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_compare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\ops.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>=' not supported between instances of 'str' and 'Timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_agg_17 \u001b[38;5;241m=\u001b[39m df_agg_17[(\u001b[43mdf_agg_17\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2017-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m&\u001b[39m (df_agg_17[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-12-31\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m      2\u001b[0m df_agg_17 \u001b[38;5;241m=\u001b[39m df_agg_17\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df_agg_17\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:1637\u001b[0m, in \u001b[0;36m_Frame._get_binary_operator.<locals>.<lambda>\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: elemwise(op, other, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1636\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other: \u001b[43melemwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py:5756\u001b[0m, in \u001b[0;36melemwise\u001b[1;34m(op, meta, out, transform_divisions, *args, **kwargs)\u001b[0m\n\u001b[0;32m   5747\u001b[0m     parts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   5748\u001b[0m         d\u001b[38;5;241m.\u001b[39m_meta\n\u001b[0;32m   5749\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_broadcastable(d)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5753\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dasks\n\u001b[0;32m   5754\u001b[0m     ]\n\u001b[0;32m   5755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m raise_on_meta_error(funcname(op)):\n\u001b[1;32m-> 5756\u001b[0m         meta \u001b[38;5;241m=\u001b[39m partial_by_order(\u001b[38;5;241m*\u001b[39mparts, function\u001b[38;5;241m=\u001b[39mop, other\u001b[38;5;241m=\u001b[39mother)\n\u001b[0;32m   5758\u001b[0m result \u001b[38;5;241m=\u001b[39m new_dd_object(graph, _name, meta, divisions)\n\u001b[0;32m   5759\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handle_out(out, result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    135\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:198\u001b[0m, in \u001b[0;36mraise_on_meta_error\u001b[1;34m(funcname, udf)\u001b[0m\n\u001b[0;32m    189\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error is below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    197\u001b[0m msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m funcname \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(e), tb)\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Metadata inference failed in `ge`.\n\nOriginal error is below:\n------------------------\nTypeError(\"'>=' not supported between instances of 'str' and 'Timestamp'\")\n\nTraceback:\n---------\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py\", line 177, in raise_on_meta_error\n    yield\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py\", line 5756, in elemwise\n    meta = partial_by_order(*parts, function=op, other=other)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\dask\\utils.py\", line 1225, in partial_by_order\n    return function(*args2, **kwargs)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\", line 72, in new_method\n    return method(self, other)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\", line 62, in __ge__\n    return self._cmp_method(other, operator.ge)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 6243, in _cmp_method\n    res_values = ops.comparison_op(lvalues, rvalues, op)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 287, in comparison_op\n    res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n  File \"C:\\Users\\Sharvari\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 75, in comp_method_OBJECT_ARRAY\n    result = libops.scalar_compare(x.ravel(), y, op)\n  File \"pandas\\_libs\\ops.pyx\", line 107, in pandas._libs.ops.scalar_compare\n"
     ]
    }
   ],
   "source": [
    "df_agg_17 = df_agg_17[(df_agg_17['date'] >= pd.to_datetime('2017-01-01')) & (df_agg_17['date'] <= pd.to_datetime('2017-12-31'))]\n",
    "df_agg_17 = df_agg_17.reset_index(drop=True)\n",
    "df_agg_17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dask dataframe to pandas dataframe\n",
    "df_jfk_17_pandas = df_jfk_17.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "# Read all parquet files for 2017 dataset\n",
    "df_18 = dd.read_parquet('yellow_tripdata_2018-*.parquet', dtype={'trip_distance': float,\n",
    "                        'total_amount': float, 'tolls_amount':float, 'RatecodeID': float, 'VendorID': float, \n",
    "                                                     'passenger_count': float, 'payment_type':float, \n",
    "                                                     'PULocationID':int, 'DOLocationID':int},\n",
    "                     usecols=['tpep_pickup_datetime', 'passenger_count', 'PULocationID', 'DOLocationID'])\n",
    "\n",
    "# Filter the records with pickup from JFK (zone 132)\n",
    "df_jfk_18 = df_18[df_18['PULocationID'] == 132]\n",
    "\n",
    "# Extract date and hour from pickup time\n",
    "df_jfk_18['date'] = df_jfk_18['tpep_pickup_datetime'].dt.date\n",
    "df_jfk_18['hour'] = df_jfk_18['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "df_jfk_18 = df_jfk_18.reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Select only the desired columns\n",
    "df_jfk_18 = df_jfk_18[['date', 'hour', 'DOLocationID', 'passenger_count']]\n",
    "\n",
    "# Convert the dask dataframe to pandas dataframe\n",
    "df_jfk_18_pandas = df_jfk_18.compute()\n",
    "\n",
    "df_agg_18 = df_jfk_18_pandas.groupby(['date', 'hour', 'DOLocationID']).sum()[['passenger_count']].reset_index()\n",
    "\n",
    "df_agg_18.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_agg_18 = df_agg_18[(df_agg_18['date'] >= pd.to_datetime('2018-01-01')) & (df_agg_18['date'] <= pd.to_datetime('2018-12-31'))]\n",
    "df_agg_18 = df_agg_18.reset_index(drop=True)\n",
    "df_agg_18.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_agg_combined = pd.concat([df_agg_17, df_agg_18])\n",
    "df_agg_combined = df_agg_combined.reset_index(drop=True)\n",
    "df_agg_combined.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_agg_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sanity check\n",
    "Then, we need to do some basic sanity checks. It is possible that in a particular hour, completely dispatched no yellow taxis from JFK. Check does each day has 24-hour records and add missing records back to the dataframe. The final output should have 17520 rows ($365\\times2\\times24$)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Get the unique dates and hours in the dataframe\n",
    "unique_dates = df_agg_combined['date'].unique()\n",
    "unique_hours = df_agg_combined['hour'].unique()\n",
    "\n",
    "# Check if each date has 24-hour records\n",
    "missing_groups = []\n",
    "for date in unique_dates:\n",
    "    for hour in unique_hours:\n",
    "        group = df_agg_combined[(df_agg_combined['date'] == date) & (df_agg_combined['hour'] == hour)]\n",
    "        if len(group) != num_locs: \n",
    "            missing_groups.append((date, hour))\n",
    "            \n",
    "# Create a new dataframe with missing date-hour combinations\n",
    "if missing_groups:\n",
    "    print(\"Missing date-hour groups:\")\n",
    "    print(missing_groups)\n",
    "    missing_df = pd.DataFrame(missing_groups, columns=['date', 'hour'])\n",
    "    missing_df['DOLocationID'] = np.arange(num_locs)\n",
    "    missing_df['passenger_count'] = 0\n",
    "\n",
    "    # Concatenate the missing dataframe with the original dataframe\n",
    "    df_agg_combined = pd.concat([df_agg_combined, missing_df])\n",
    "    df_agg_combined = df_agg_combined.groupby(['date', 'hour', 'DOLocationID']).sum().reset_index()\n",
    "\n",
    "# Sort the dataframe by date and hour\n",
    "df_agg_combined = df_agg_combined.sort_values(by=['date', 'hour']).reset_index(drop=True)\n",
    "\n",
    "# Check the final length of the dataframe\n",
    "assert len(df_agg_combined) == len(unique_dates) * len(unique_hours) * num_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of drop-off locations\n",
    "#num_locs = 263\n",
    "num_locs = len(df_agg_17['DOLocationID'].unique())\n",
    "\n",
    "# Subset the 2017 dataframe to include only records from January 1 to December 31\n",
    "df_agg_17 = df_agg_17[(df_agg_17['date'] >= pd.to_datetime('2017-01-01')) & (df_agg_17['date'] <= pd.to_datetime('2017-12-31'))]\n",
    "\n",
    "# Reset the index of the dataframe\n",
    "df_agg_17 = df_agg_17.reset_index(drop=True)\n",
    "\n",
    "# Check if each date has 24-hour records\n",
    "unique_dates = df_agg_17['date'].unique()\n",
    "unique_hours = df_agg_17['hour'].unique()\n",
    "missing_groups = []\n",
    "for date in unique_dates:\n",
    "    for hour in unique_hours:\n",
    "        group = df_agg_17[(df_agg_17['date'] == date) & (df_agg_17['hour'] == hour)]\n",
    "        if len(group) != num_locs: \n",
    "            missing_groups.append((date, hour))\n",
    "\n",
    "# Create a new dataframe with missing date-hour combinations\n",
    "if missing_groups:\n",
    "    print(\"Missing date-hour groups:\")\n",
    "    print(missing_groups)\n",
    "    missing_df = pd.DataFrame(missing_groups, columns=['date', 'hour'])\n",
    "    missing_df['DOLocationID'] = np.arange(num_locs)\n",
    "    missing_df['passenger_count'] = 0\n",
    "\n",
    "    # Concatenate the missing dataframe with the original dataframe\n",
    "    df_agg_17 = pd.concat([df_agg_17, missing_df])\n",
    "    df_agg_17 = df_agg_17.groupby(['date', 'hour', 'DOLocationID']).sum().reset_index()\n",
    "\n",
    "# Sort the dataframe by date and hour\n",
    "df_agg_17 = df_agg_17.sort_values(by=['date', 'hour']).reset_index(drop=True)\n",
    "\n",
    "# Check the final length of the dataframe\n",
    "assert len(df_agg_17) == len(unique_dates) * len(unique_hours) * num_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-series exploratory analysis\n",
    "Apply exploratory analysis over the daily aggregated dataset at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 aggregate the ridership from each dropoff location and sum it to get daily records. (3pts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_daily = df_agg_combined.groupby('date')['passenger_count'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df_daily = df_agg_17.groupby('date')['passenger_count'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Period detection and report the strongest period length on the 2017 data. (3pts)\n",
    "Hint: using periodogram or acf plot."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "# Plot the autocorrelation function\n",
    "plot_acf(ridership, lags=365)\n",
    "plt.xlabel('Lag (days)')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from scipy.signal import periodogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get data for 2017\n",
    "df_2017 = df_agg_daily[df_agg_daily['date'].dt.year == 2017]\n",
    "ridership = df_2017.groupby('date')['passenger_count'].sum().values\n",
    "\n",
    "# Calculate the spectral density\n",
    "freq, spec = periodogram(ridership)\n",
    "\n",
    "# Plot the periodogram\n",
    "plt.plot(freq, spec)\n",
    "plt.xlabel('Frequency (cycles/day)')\n",
    "plt.ylabel('Spectral Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Trend, seasonality, noise decomposition (using additive model) on 2017 data, . (3 pts, 1 pts if freq is wrong)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Set the frequency to daily\n",
    "freq = 'D'\n",
    "\n",
    "# Create a new dataframe with only the 2017 data\n",
    "df_2017 = df_agg_daily.loc['2017-01-01':'2017-12-31']\n",
    "\n",
    "# Perform the decomposition\n",
    "decomposition = seasonal_decompose(df_2017['passenger_count'], model='additive', freq=freq)\n",
    "\n",
    "# Plot the decomposition\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "decomposition.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict the total daily ridership from JFK using ARIMA.\n",
    "ARIMA is a common method to predict taxi ridership. Before we predict taxi zone level hourly ridership, let's try to predict the aggregated daily ridership using ARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Using adfuller test to test the stability of the aggregated dataset. If not stable, apply differencing method until the p-value from adfuller test is smaller than 0.05. (3pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Find out proper AR and MA terms in an ARIMA model using pacf and acf plots. (4 pts, 2 for each plot)\n",
    "Hint: positive autocorrelation is usually best treated by adding an AR term to the model and negative autocorrelation is usually best treated by adding an MA term. In general, differencing reduces positive autocorrelation and may even cause a switch from positive to negative autocorrelation. \n",
    "\n",
    "Identifying the numbers of AR and MA terms:\n",
    "1. if the pacf plot shows a sharp cutoff and/or the lag-1 autocorrelation is positive then consider adding one or more AR terms to the model. The lag beyond which the PACF cuts off is the indicated number of AR terms.\n",
    "\n",
    "2. if the acf plot displays a sharp cutoff and/or the lag-1 autocorrelation is negative then consider adding an MA term to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms.\n",
    "\n",
    "3. It is generally advisable to stick to models in which at least one of p and q is no larger than 1, i.e., do not try to fit a model such as ARIMA(2,1,2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 build an ARIMA model using terms from 4.2, training on the first 700 days, forecast on the last 31 days. Print ARIMA model results and plot in-sample and out-of-sample prediction in different colors. (8 pts, 3 for correct terms, 3 for training and summary, 2pts for the plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi zone level prediction\n",
    "\n",
    "This project aims to predict hourly yellow taxi ridership volume from JFK to each taxi zone. The ARIMA experiment in section 3 forecasts the total ridership amount from JFK. However, based on the reported $R^2$, this model is not a good fit. ARIMA model has four main shortcomings: 1) they rely heavily on stationarity assumption which does not hold in real-world traffic systems 2) they do not consider spatial and structural dependencies that traffic networks exhibit and forecast each sensor as an individual time series 3) they are unable to model non-linear temporal dynamics 4) they suffer from the curse of dimensionality. Due to the limitation of ARIMA, we need to apply another method to predict taxi zone level ridership."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature engineering\n",
    "\n",
    "Our workflow is first standardizing the dataset, then using PCA to compress the dataset. As we predict future ridership, PCA should be learned from historical data (2017) then apply to the following year (2018). Next, add lag features (PCA components) from the past 12 hours and apply a Random Forest regressor to predict each PCA component's values in the next hour. After we had the PCA component prediction, inverse PCA, and inverse standardization to retrieve taxi ridership prediction in its original scale and dimension, in other words, we are predicting the PCA components instead of taxi zone level ridership and then using the inverse PCA method to reconstruct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. standardization. (3 pts)\n",
    "The standardscaler stores information of this standardization process, including the mean and standard deviation values required when converting the prediction back to the raw scale. Split the whole dataset into two parts: 2017 and 2018, standardize each separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. PCA\n",
    "\n",
    "#### 5.2.1 train PCA on 2017 data. Let's arbitrarily set PCA components as 5, and gamma is None, try kernel ‘linear’, ‘poly’, ‘rbf’, and ‘sigmoid’. Select the transformer which has the lowest mean squared error in data reconstruction (inverse transform). (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Apply the selected transformer from 4.2.1 to the standardized 2018 dataset and report the mean squared error between the standardized data and reconstructed data. Hint: fit the PCA on 2017 data and apply it to transform 2018 data.(5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Add lag (5pts)\n",
    "add 12 lags of each component from 4.2.2 (compressed 2018 data only). The expected output should have 65 dimensions. In the further modeling step, we will apply the 60 lag variables to predict the 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RandomForest modeling (23pts)\n",
    "\n",
    "We aim at predicting compressed daily ridership (5 PCA components values) from 12-hour lag variables. Parameter tuning is required in this section, including min_samples_split, min_samples_leaf, and n_estimators. First 80% days for training, test on the rest 20%. And in the training dataset, validate the model on the bottom 20%. \n",
    "\n",
    "\n",
    "### Extra credit: \n",
    "\n",
    "Using grid_search function in sklearn instead of a for-loop when tuning parameters in a RandomForest. The train, validation, and test datasets should be split in the same way as described above. Hint: To fix train and validation in a grid search, you might need the PredefinedSplit function from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 train test split (3pts)\n",
    "Please keep in mind that random train test split is not applicable in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 parameter tuning (10pts)\n",
    "Please search the best parameter set in the following range:\n",
    "min_samples_split: 2 to 10,\n",
    "min_samples_leaf: 2 to 10,\n",
    "and n_estimators equal to 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 model performance measurement (10pts)\n",
    "Prediction results are PCA components instead of taxi zone level ridership. To reconstruct the data back to its original size and scale, we need to inverse PCA and inverse standardization. Report the taxi zone level $R^2$ value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
